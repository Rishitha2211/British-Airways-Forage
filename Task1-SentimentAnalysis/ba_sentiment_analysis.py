# -*- coding: utf-8 -*-
"""BA_sentiment_analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HDJZl4fq4vJNKhmAIedT6R-2Ad54ivYm

# Imports
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import re
import nltk
#nltk.download('stopwords') for downloading the stopwords
from nltk.corpus import wordnet
from transformers import BertTokenizer, TFBertForSequenceClassification
import numpy as np
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from collections import Counter
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

import nltk
nltk.download('punkt')
from nltk.corpus import wordnet
import nltk
nltk.download('stopwords')
import nltk
nltk.download('punkt_tab')

!pip install contractions

!pip install vaderSentiment

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer







"""# Web scraping"""

base_url = "https://www.airlinequality.com/airline-reviews/british-airways"
pages = 10
page_size = 100

reviews = []
rating = []
postdate = []
for i in range(1, pages + 1):

    print(f"Scraping page {i}")

    url = f"{base_url}/page/{i}/?sortby=post_date%3ADesc&pagesize={page_size}"

    response = requests.get(url)

    content = response.content
    parsed_content = BeautifulSoup(content, 'html.parser')
    for para in parsed_content.find_all("div", {"class": "text_content"}):
        reviews.append(para.get_text())
    for para in parsed_content.find_all("span", {"itemprop": "ratingValue"}):
        rating.append(para.get_text())
    for para in parsed_content.find_all("time", {"itemprop": "datePublished"}):
        postdate.append(para.get_text())

item_to_remove = '\n\t\t\t\t\t\t\t\t\t\t\t\t5'

# Modify the original list (use with caution)
while item_to_remove in rating:
  rating.remove(item_to_remove)

len(reviews)

data=pd.DataFrame()

data['review']=reviews
data['rating']=rating
data['postdate']=postdate

"""# Preprocessing"""

data

def extract_verified_status(review):
    words = review.split()
    if len(words) >= 2 and " ".join(words[:2]).lower() == "not verified":
        return 0, " ".join(words[2:])  # Mark as 0 for Not Verified
    return 1, " ".join(words[3:]) if len(words) > 2 else ""  # Mark as 1 otherwise

# Apply function to extract verified status and update dataframe
data[["Verified User Review", "preprocessed"]] = data["review"].apply(lambda x: pd.Series(extract_verified_status(x)))

data.info()

import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

stop_words = set(stopwords.words('english'))
additional_stopwords = ["airport", "flight", "british", "airways", "airline", "london", "heathrow", "ba"] # Example additional words
stop_words.update(additional_stopwords)
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    # Convert to lowercase
    text = text.lower()

    # Remove non-alphanumeric characters and extra whitespace
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()

    # Tokenize the text
    tokens = word_tokenize(text)

    # Remove stop words and lemmatize
    processed_tokens = [lemmatizer.lemmatize(w) for w in tokens if not w in stop_words]

    return " ".join(processed_tokens)

# Apply preprocessing to the 'preprocessed' column
data['preprocessed'] = data['preprocessed'].apply(preprocess_text)
data.head()

data['preprocessed'][4]

import contractions

def expand_contractions(text):
    expanded_words = []
    for word in text.split():
        expanded_words.append(contractions.fix(word))
    expanded_text = ' '.join(expanded_words)
    return expanded_text

# Example usage (assuming 'data' DataFrame exists from previous code)
data['preprocessed1'] = data['preprocessed'].apply(expand_contractions)
data.head()

data['preprocessed1'][4]

def Negation(sentence):
  '''
  Input: Tokenized sentence (List of words)
  Output: Tokenized sentence with negation handled (List of words)
  '''
  sentence = sentence.split()
  temp = int(0)
  for i in range(len(sentence)):
      if sentence[i-1] in ['not',"n't"]:
          antonyms = []
          for syn in wordnet.synsets(sentence[i]):
              syns = wordnet.synsets(sentence[i])
              w1 = syns[0].name()
              temp = 0
            # Dropping unnecessary tags and creating list of antonyms
              for l in syn.lemmas():
                  if l.antonyms():
                      antonyms.append(l.antonyms()[0].name())
              max_dissimilarity = 0
            # Comparing every antonym with synonym word and calculating dissimilarity
              for ant in antonyms:
                  syns = wordnet.synsets(ant)
                  w2 = syns[0].name()
                  syns = wordnet.synsets(sentence[i])
                  w1 = syns[0].name()
                  word1 = wordnet.synset(w1)
                  word2 = wordnet.synset(w2)
                # wup_similarity : Calculates the similarity between two words.
                  if isinstance(word1.wup_similarity(word2), float) or isinstance(word1.wup_similarity(word2), int):
                      temp = 1 - word1.wup_similarity(word2)
                 # Considering the most dissimilar antonym
                  if temp>max_dissimilarity:
                      max_dissimilarity = temp
                      antonym_max = ant
                      sentence[i] = antonym_max
                      sentence[i-1] = ''
  while '' in sentence:
      sentence.remove('')
  return " ".join(sentence)
data['preprocessed2']=data['preprocessed1'].apply(Negation)
data.head()

data['preprocessed2'][4]

import re
import gensim
from gensim.models import Phrases
from gensim.models.phrases import Phraser

def handle_negation(text):
    text = re.sub(r"\b(not|no|n't)\s+(\w+)", r"\1_\2", text)
    return text

# Apply negation handling
data['preprocessed3'] = data['preprocessed1'].apply(handle_negation)

# Convert to list of tokens
tokenized_texts = [text.split() for text in data['preprocessed3']]

# Apply bigram detection to catch more phrases
bigram = Phrases(tokenized_texts, min_count=5, threshold=10)
bigram_mod = Phraser(bigram)

# Transform text
data['preprocessed4'] = data['preprocessed3'].apply(lambda x: " ".join(bigram_mod[x.split()]))

data['preprocessed3'][4]

data['preprocessed4'][4]

"""Topic modeling and Word clouds"""

# Number of topics
num_topics = 3

# Create a CountVectorizer object
vectorizer = CountVectorizer(max_df=0.95, min_df=3, stop_words='english')

# Fit and transform the preprocessed text data
dtm = vectorizer.fit_transform(data['preprocessed4'])

# Create an LDA model
LDA = LatentDirichletAllocation(n_components=num_topics, random_state=42)

# Fit the LDA model to the document-term matrix
LDA.fit(dtm)

# Get the top words for each topic
for index,topic in enumerate(LDA.components_):
    print(f'THE TOP 15 WORDS FOR TOPIC #{index}')
    print([vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]])
    print('\n')

# Assign topics to documents
topic_results = LDA.transform(dtm)

# Add topic assignments to the DataFrame
data['Topic'] = topic_results.argmax(axis=1)

# Display the updated DataFrame with topic assignments
#print(data.head())

import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Filter reviews containing "customer service"
customer_service_reviews = data[data['preprocessed4'].str.contains("customer_service", na=False)]

# Combine all preprocessed reviews into a single string
text = " ".join(customer_service_reviews['preprocessed4'].tolist())

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

# Display the word cloud
plt.figure(figsize=(10, 5), facecolor=None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()

"""when looking at reviews about customer service, customers were talking about:


*   delay in responses.
*   refunds, vouchers.
*   Reebooking and cancellations.





"""

# Filter reviews containing "cabin_crew"
cabin_crew_reviews = data[data['preprocessed4'].str.contains("cabin_crew", na=False)]

# Combine all preprocessed reviews into a single string
text = " ".join(cabin_crew_reviews['preprocessed4'].tolist())

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

# Display the word cloud
plt.figure(figsize=(10, 5), facecolor=None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()

"""when looking at reviews about cabin crew, customers were talking about:


*   service being quick and  friendly.
*   having good food, comforatble seating.
* nice lounge and boarding experience.

"""

from collections import Counter
import pandas as pd


def find_frequent_bigrams(text_column, top_n=10):
    """
    Finds the most frequent bigrams (two words appearing together) in a text column.
    """
    bigrams = []
    for text in text_column:
        words = text.split()
        for i in range(len(words) - 1):
            bigrams.append(tuple(words[i:i+2]))  # Store bigrams as tuples

    bigram_counts = Counter(bigrams)
    return bigram_counts.most_common(top_n)

# Find the top 10 most frequent bigrams in the 'preprocessed4' column
frequent_bigrams = find_frequent_bigrams(data['preprocessed4'], top_n=10)

# Print the results
print("Top 10 most frequent bigrams:")
for bigram, count in frequent_bigrams:
    print(f"{bigram}: {count}")

"""Most seen words in the reviews are about

*   business class seating.
*   good food and service.
*   boarding time.

# Sentiment Analysis
"""

analyzer = SentimentIntensityAnalyzer()

def analyze_sentiment(text):
    scores = analyzer.polarity_scores(text)
    compound_score = scores['compound']

    if compound_score >= 0.05:
        sentiment = 'Positive'
    elif compound_score <= 0.01:
        sentiment = 'Negative'
    else:
        sentiment = 'Neutral'
    return sentiment, compound_score

# Apply sentiment analysis
data[['Sentiment', 'Compound Score']] = data['preprocessed4'].apply(lambda x: pd.Series(analyze_sentiment(x)))

#  sentiments by rating

import matplotlib.pyplot as plt

# Assuming 'data' DataFrame with 'Sentiment' and 'rating' columns exists
# ... (Your existing code)

# Convert 'rating' column to numeric, handling potential errors
data['rating'] = pd.to_numeric(data['rating'], errors='coerce')

# Group data by rating and sentiment, count occurrences
sentiment_by_rating = data.groupby(['rating', 'Sentiment']).size().unstack(fill_value=0)

# Create the bar graph
sentiment_by_rating.plot(kind='bar', figsize=(10, 6))
plt.title('Sentiments by Rating')
plt.xlabel('Rating')
plt.ylabel('Number of Reviews')
plt.xticks(rotation=0)  # Rotate x-axis labels for better readability
plt.legend(title='Sentiment')
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Assuming 'data' DataFrame with 'Sentiment' column exists
# ... (Your existing code)

# Count the occurrences of each sentiment
sentiment_counts = data['Sentiment'].value_counts()

# Create the pie chart
plt.figure(figsize=(4, 4))
plt.pie(sentiment_counts, labels=sentiment_counts.index, autopct='%1.1f%%', startangle=90)
plt.title('Distribution of Sentiments')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.show()

import matplotlib.pyplot as plt

# Assuming 'data' DataFrame with 'Sentiment' and 'Verified User Review' columns exists
# ... (Your existing code)

# Convert 'Verified User Review' column to numeric (0 or 1)
data['Verified User Review'] = pd.to_numeric(data['Verified User Review'], errors='coerce')

# Group data by verified status and sentiment, count occurrences
sentiment_by_verified = data.groupby(['Verified User Review', 'Sentiment']).size().unstack(fill_value=0)

# Create the bar chart
sentiment_by_verified.plot(kind='bar', figsize=(10, 6))
plt.title('Sentiment by Verified Status')
plt.xlabel('Verified User (0: Not Verified, 1: Verified)')
plt.ylabel('Number of Reviews')
plt.xticks(rotation=0)  # Rotate x-axis labels for better readability
plt.legend(title='Sentiment')
plt.tight_layout()
plt.show()

# prompt: word cloud on preprocessed4 where sentiment =positive

import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Assuming 'data' and 'preprocessed4' column exist from the previous code
# Filter for positive sentiment
positive_reviews = data[data['Sentiment'] == 'Positive']

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(" ".join(positive_reviews['preprocessed4']))

# Display the word cloud
plt.figure(figsize=(10, 5), facecolor=None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()

"""Positive reviews talk about good:


*   seating.
*   service in plane, staff, crew.
*   Food, Lounge.
*   Business class



"""

# prompt: word cloud on preprocessed4 where sentiment =positive

import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Assuming 'data' and 'preprocessed4' column exist from the previous code
# Filter for positive sentiment
positive_reviews = data[data['Sentiment'] == 'Negative']

# Generate the word cloud
wordcloud = WordCloud(width=800, height=400, background_color='white').generate(" ".join(positive_reviews['preprocessed4']))

# Display the word cloud
plt.figure(figsize=(10, 5), facecolor=None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()

"""Negative reviews talk about not better:

* customer service response time.
* Baggage service.
* Refunds and rebooking.
* Delays and cancellations
"""